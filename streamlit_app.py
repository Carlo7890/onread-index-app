import streamlit as st
import pandas as pd
import re
from PIL import Image
from kiwipiepy import Kiwi
import base64
import requests
import os
import datetime

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi()

@st.cache_data
def load_vocab():
    vocab_file = "ì‚¬ê³ ë„êµ¬ì–´(1~4ë“±ê¸‰)(ê°€ê³µ).xlsx"
    sheets = pd.read_excel(vocab_file, sheet_name=None)
    word_dict = {}
    for level, df in sheets.items():
        for word in df["ë‹¨ì–´ì¡±"]:
            word_dict[str(word).strip()] = int(level[0])
    return word_dict

@st.cache_data
def load_grade_ranges():
    df = pd.read_excel("ì˜¨ë…ì§€ìˆ˜ë²”ìœ„.xlsx")
    ranges = []
    for _, row in df.iterrows():
        start, end = map(int, row["ì˜¨ë…ì§€ìˆ˜ ë²”ìœ„"].split("~"))
        ranges.append((start, end, row["ëŒ€ìƒ í•™ë…„"]))
    return ranges

def call_vision_api(image_bytes):
    api_key = st.secrets["vision_api_key"]
    url = f"https://vision.googleapis.com/v1/images:annotate?key={api_key}"

    image_base64 = base64.b64encode(image_bytes).decode("utf-8")
    request_body = {
        "requests": [
            {
                "image": {"content": image_base64},
                "features": [{"type": "TEXT_DETECTION"}]
            }
        ]
    }

    response = requests.post(url, json=request_body)
    if response.status_code == 200:
        result = response.json()
        try:
            return result["responses"][0]["fullTextAnnotation"]["text"]
        except:
            return ""
    else:
        st.error("Google Vision API ìš”ì²­ ì‹¤íŒ¨: " + response.text)
        return ""

def calculate_onread_index(text, vocab_dict, grade_ranges):
    analyzed = kiwi.analyze(text)
    tokens = [token.lemma for token in analyzed[0][0] if token.tag in ('NNG', 'NNP', 'VV', 'VA', 'MAG', 'MM')]

    token_counts = {}
    total = 0
    weighted_sum = 0
    used_words = []
    seen_words = set()
    counted_tokens = set()

    for token in tokens:
        if token in vocab_dict and token not in counted_tokens:
            level = vocab_dict[token]
            token_counts[level] = token_counts.get(level, 0) + 1
            weighted_sum += level
            total += 1
            used_words.append((token, level))
            seen_words.add(token)
            counted_tokens.add(token)

    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", [], 0, 0

    unique = len(seen_words)
    cttr = unique / (2 * total) ** 0.5
    cttr = min(cttr, 1.0)

    norm_weighted = weighted_sum / (4 * total)
    total_words = len(re.findall(r"[\wê°€-í£]+", text))
    density = total / total_words if total_words > 0 else 0

    density_factor = 0.5 + 0.5 * density
    index = ((0.7 * cttr + 0.3 * norm_weighted) * 500 + 100) * density_factor

    matched_levels = [grade for start, end, grade in grade_ranges if start <= index < end]
    if not matched_levels:
        level = "í•´ì„ ë¶ˆê°€"
    elif len(matched_levels) == 1:
        level = matched_levels[0]
    else:
        level = f"{matched_levels[0]}~{matched_levels[-1]}"

    return round(index), level, used_words, total, total_words

# âœ… API í˜¸ì¶œ íšŸìˆ˜ ì œí•œ ë¡œì§ ì¶”ê°€
if "daily_calls" not in st.session_state:
    st.session_state["daily_calls"] = 0
    st.session_state["last_reset"] = datetime.date.today()

if st.session_state["last_reset"] != datetime.date.today():
    st.session_state["daily_calls"] = 0
    st.session_state["last_reset"] = datetime.date.today()

if st.session_state["daily_calls"] >= 20:
    st.error("ì˜¤ëŠ˜ì˜ í• ë‹¹ëŸ‰ì„ ëª¨ë‘ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    st.stop()

st.title("ğŸ“˜ ì˜¨ë…ì§€ìˆ˜ ìë™ ë¶„ì„ê¸°")

vocab_dict = load_vocab()
grade_ranges = load_grade_ranges()

input_method = st.radio("ì…ë ¥ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:", ("ë¬¸ì¥ ì§ì ‘ ì…ë ¥", "ì´ë¯¸ì§€ ì—…ë¡œë“œ"))
text = ""
trigger = False

if input_method == "ë¬¸ì¥ ì§ì ‘ ì…ë ¥":
    text = st.text_area("ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”", key="manual_text")
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True
elif input_method == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
    uploaded_file = st.file_uploader("ë¬¸ì¥ì´ ë‹´ê¸´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg"])
    ocr_text = ""
    if uploaded_file:
        try:
            image_bytes = uploaded_file.read()
            image = Image.open(uploaded_file)
            st.image(image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
            ocr_text = call_vision_api(image_bytes).strip()
            st.session_state["daily_calls"] += 1  # API í˜¸ì¶œ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
            st.session_state["ocr_text"] = ocr_text  # OCR ê²°ê³¼ ì €ì¥
        except Exception as e:
            st.error(f"ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    text = st.text_area("ğŸ“ ì¸ì‹ëœ í•œê¸€ í…ìŠ¤íŠ¸ (ìˆ˜ì • ê°€ëŠ¥):", value=st.session_state.get("ocr_text", ""), key="ocr_text_area", height=150)
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True

if trigger:
    current_text = st.session_state.get("manual_text") if input_method == "ë¬¸ì¥ ì§ì ‘ ì…ë ¥" else st.session_state.get("ocr_text_area")
    if current_text:
        score, level, used_words, total_count, total_words = calculate_onread_index(current_text, vocab_dict, grade_ranges)
        if score == 0:
            st.warning(level)
        else:
            st.success(f"âœ… ì˜¨ë…ì§€ìˆ˜: {score}ì  ({level})")
            st.caption(f"(ì´ ë‹¨ì–´ ìˆ˜: {total_words} / ì‚¬ê³ ë„êµ¬ì–´ ìˆ˜: {total_count})")
            if total_count < 3:
                st.info("â„¹ï¸ ë¬¸ì¥ì´ ì§§ì•„ ì‚¬ê³ ë„êµ¬ì–´ ìˆ˜ê°€ ì ì§€ë§Œ, ê²°ê³¼ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.")
            if score > 500:
                st.info("ğŸ’¡ ì˜¨ë…ì§€ìˆ˜ê°€ ê³ 3 ìˆ˜ì¤€(500ì )ì„ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë§¤ìš° ë†’ì€ ìˆ˜ì¤€ì˜ ì‚¬ê³ ë„êµ¬ì–´ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            if used_words:
                st.markdown("### ì‚¬ìš©ëœ ì‚¬ê³ ë„êµ¬ì–´ ëª©ë¡")
                for word, lvl in used_words:
                    st.markdown(f"- **{word}**: {lvl}ë“±ê¸‰")
    else:
        st.warning("â— ë¬¸ì¥ì„ ì…ë ¥í•œ ë’¤ ë¶„ì„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

        "requests": [
            {
                "image": {"content": image_base64},
                "features": [{"type": "TEXT_DETECTION"}]
            }
        ]
    }

    response = requests.post(url, json=request_body)
    if response.status_code == 200:
        result = response.json()
        try:
            return result["responses"][0]["fullTextAnnotation"]["text"]
        except:
            return ""
    else:
        st.error("Google Vision API ìš”ì²­ ì‹¤íŒ¨: " + response.text)
        return ""

def calculate_onread_index(text, vocab_dict, grade_ranges):
    analyzed = kiwi.analyze(text)
    tokens = [token.form for token in analyzed[0][0] if token.tag in ('NNG', 'NNP', 'VV', 'VA', 'MAG', 'MM')]

    token_counts = {}
    total = 0
    weighted_sum = 0
    used_words = []
    seen_words = set()

    for token in tokens:
        if token in vocab_dict:
            level = vocab_dict[token]
            token_counts[level] = token_counts.get(level, 0) + 1
            weighted_sum += level
            total += 1
            if token not in seen_words:
                used_words.append((token, level))
                seen_words.add(token)

    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", [], 0, 0

    unique = len(seen_words)
    cttr = unique / (2 * total) ** 0.5
    cttr = min(cttr, 1.0)

    norm_weighted = weighted_sum / (4 * total)
    total_words = len(re.findall(r"[\wê°€-í£]+", text))
    density = total / total_words if total_words > 0 else 0

    density_factor = 0.5 + 0.5 * density
    index = ((0.7 * cttr + 0.3 * norm_weighted) * 500 + 100) * density_factor

    matched_levels = [grade for start, end, grade in grade_ranges if start <= index < end]
    if not matched_levels:
        level = "í•´ì„ ë¶ˆê°€"
    elif len(matched_levels) == 1:
        level = matched_levels[0]
    else:
        level = f"{matched_levels[0]}~{matched_levels[-1]}"

    return round(index), level, used_words, total, total_words

# âœ… API í˜¸ì¶œ íšŸìˆ˜ ì œí•œ ë¡œì§ ì¶”ê°€
if "daily_calls" not in st.session_state:
    st.session_state["daily_calls"] = 0
    st.session_state["last_reset"] = datetime.date.today()

if st.session_state["last_reset"] != datetime.date.today():
    st.session_state["daily_calls"] = 0
    st.session_state["last_reset"] = datetime.date.today()

if st.session_state["daily_calls"] >= 20:
    st.error("ì˜¤ëŠ˜ì˜ í• ë‹¹ëŸ‰ì„ ëª¨ë‘ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    st.stop()

st.title("ğŸ“˜ ì˜¨ë…ì§€ìˆ˜ ìë™ ë¶„ì„ê¸°")

vocab_dict = load_vocab()
grade_ranges = load_grade_ranges()

input_method = st.radio("ì…ë ¥ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:", ("ë¬¸ì¥ ì§ì ‘ ì…ë ¥", "ì´ë¯¸ì§€ ì—…ë¡œë“œ"))
text = ""
trigger = False

if input_method == "ë¬¸ì¥ ì§ì ‘ ì…ë ¥":
    text = st.text_area("ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”")
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True
elif input_method == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
    uploaded_file = st.file_uploader("ë¬¸ì¥ì´ ë‹´ê¸´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg"])
    ocr_text = ""
    if uploaded_file:
        try:
            image_bytes = uploaded_file.read()
            image = Image.open(uploaded_file)
            st.image(image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
            ocr_text = call_vision_api(image_bytes).strip()
            st.session_state["daily_calls"] += 1  # API í˜¸ì¶œ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
        except Exception as e:
            st.error(f"ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    text = st.text_area("ğŸ“ ì¸ì‹ëœ í•œê¸€ í…ìŠ¤íŠ¸ (ìˆ˜ì • ê°€ëŠ¥):", value=ocr_text, height=150)
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True

if trigger and text:
    score, level, used_words, total_count, total_words = calculate_onread_index(text, vocab_dict, grade_ranges)
    if score == 0:
        st.warning(level)
    else:
        st.success(f"âœ… ì˜¨ë…ì§€ìˆ˜: {score}ì  ({level})")
        st.caption(f"(ì´ ë‹¨ì–´ ìˆ˜: {total_words} / ì‚¬ê³ ë„êµ¬ì–´ ìˆ˜: {total_count})")
        if total_count < 3:
            st.info("â„¹ï¸ ë¬¸ì¥ì´ ì§§ì•„ ì‚¬ê³ ë„êµ¬ì–´ ìˆ˜ê°€ ì ì§€ë§Œ, ê²°ê³¼ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.")
        if score > 500:
            st.info("ğŸ’¡ ì˜¨ë…ì§€ìˆ˜ê°€ ê³ 3 ìˆ˜ì¤€(500ì )ì„ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë§¤ìš° ë†’ì€ ìˆ˜ì¤€ì˜ ì‚¬ê³ ë„êµ¬ì–´ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        if used_words:
            st.markdown("### ì‚¬ìš©ëœ ì‚¬ê³ ë„êµ¬ì–´ ëª©ë¡")
            for word, lvl in used_words:
                st.markdown(f"- **{word}**: {lvl}ë“±ê¸‰")

