import streamlit as st
import pandas as pd
import re
import base64
import requests
from PIL import Image

# ğŸ” ì¤‘ì˜ì–´ ì •ì˜
ambiguous_meanings = {
    "ê¸°ìˆ ": {"2": "ì‚¬ë¬¼ì„ ì˜ ë‹¤ë£° ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‚˜ ëŠ¥ë ¥ (ê¸°ëŠ¥/ë°©ë²•)", "3": "ì—´ê±°í•˜ê±°ë‚˜ ê¸°ë¡í•˜ì—¬ ì„œìˆ í•¨ (ê¸°ë¡/ì„œìˆ )"},
    "ìœ í˜•": {"2": "ì„±ì§ˆì´ë‚˜ íŠ¹ì§• ë”°ìœ„ê°€ ê³µí†µì ì¸ ê²ƒë¼ë¦¬ ë¬¶ì€ í•˜ë‚˜ì˜ í‹€", "3": "í˜•ìƒì´ê±°ë‚˜ í˜•ì²´ê°€ ìˆìŒ"},
    "ì˜ì§€": {"2": "ì´ë£¨ê³ ì í•˜ëŠ” ë§ˆìŒ(ê²°ì‹¬)", "3": "ê¸°ëŒ€ë‹¤ (ì˜ì§€í•˜ë‹¤)"},
    "ì§€ì ": {"2": "ì§€ì‹œ/ì§€ëª©", "3": "ì§€ì‹ì´ë‚˜ ì§€ì„±ì— ê´€í•œ ê²ƒ"}
}
ambiguous_words = set(ambiguous_meanings.keys())

prefixes = ["ë¹„", "ë¯¸", "ë¶ˆ", "ë¬´", "ë°˜", "ë¶€", "íƒˆ", "ë¹„é"]  # ì ‘ë‘ì‚¬ ëª©ë¡

# ğŸ” ì‚¬ê³ ë„êµ¬ì–´ ì‚¬ì „ ë¡œë”©
@st.cache_data
def load_vocab():
    df = pd.read_csv("ì‚¬ê³ ë„êµ¬ì–´(1~4ë“±ê¸‰)(ê°€ê³µ).csv", encoding="utf-8-sig")
    vocab = {}
    for col, level in zip(df.columns, range(1, 5)):
        for word in df[col].dropna():
            vocab[word.strip()] = level
    return vocab

# ğŸ” ì˜¨ë…ì§€ìˆ˜ ë“±ê¸‰ ë²”ìœ„ ë¡œë”©
@st.cache_data
def load_grade_ranges():
    df = pd.read_csv("ì˜¨ë…ì§€ìˆ˜ë²”ìœ„.csv", encoding="utf-8-sig")
    ranges = []
    for _, row in df.iterrows():
        try:
            start, end = map(int, str(row["ì˜¨ë…ì§€ìˆ˜ ë²”ìœ„"]).split("~"))
            ranges.append((start, end, row["ëŒ€ìƒ í•™ë…„"]))
        except:
            continue
    return ranges

# ğŸ” Vision API OCR í˜¸ì¶œ
def call_vision_api(image_bytes):
    api_key = st.secrets["vision_api_key"]
    url = f"https://vision.googleapis.com/v1/images:annotate?key={api_key}"
    image_base64 = base64.b64encode(image_bytes).decode("utf-8")
    response = requests.post(url, json={
        "requests": [{"image": {"content": image_base64}, "features": [{"type": "TEXT_DETECTION"}]}]
    })
    try:
        return response.json()["responses"][0]["fullTextAnnotation"]["text"]
    except:
        return ""

# ğŸ” ì˜¨ë…ì§€ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (kiwi ì—†ì´)
def calculate_onread_index(text, vocab_dict, grade_ranges, user_choices=None):
    sorted_vocab = sorted(vocab_dict.items(), key=lambda x: -len(x[0]))
    tokens = re.findall(r"\b[\wê°€-í£]+\b", text)

    seen, used, total, weighted = set(), [], 0, 0

    for token in tokens:
        # 1. ì‚¬ìš©ì ì„ íƒí•œ ì¤‘ì˜ì–´ ìš°ì„ 
        if token in ambiguous_words and user_choices and token in user_choices:
            level = user_choices[token]
            if token not in seen:
                seen.add(token)
                used.append((token, level))
                total += 1
                weighted += level
            continue

        # 2. ì •í™• ì¼ì¹˜
        matched = False
        for word, level in sorted_vocab:
            if token == word and word not in seen:
                seen.add(word)
                used.append((word, level))
                total += 1
                weighted += level
                matched = True
                break
        if matched:
            continue

        # 3. ì ‘ë‘ì‚¬ ì œê±° í›„ ì–´ê·¼ ë§¤ì¹­
        for prefix in prefixes:
            if token.startswith(prefix):
                stem = token[len(prefix):]
                if stem in vocab_dict and stem not in seen:
                    level = user_choices[stem] if stem in ambiguous_words and user_choices and stem in user_choices else vocab_dict[stem]
                    seen.add(stem)
                    used.append((stem, level))
                    total += 1
                    weighted += level
                    break

    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", [], 0, 0

    word_tokens = re.findall(r"[\wê°€-í£]+", text)
    cttr = min(len(seen) / (2 * total) ** 0.5, 1.0)
    norm_weight = weighted / (4 * total)
    density = total / len(word_tokens)
    index = ((0.7 * cttr + 0.3 * norm_weight) * 500 + 100) * (0.5 + 0.5 * density)

    if len(word_tokens) < 5:
        index *= 0.6

    matched = [g for s, e, g in grade_ranges if s <= index < e]
    level = "~".join(matched) if matched else "í•´ì„ ë¶ˆê°€"
    return round(index), level, used, total, len(word_tokens)

# âœ… Streamlit ì•± ì‹œì‘
st.title("ğŸ“˜ ì˜¨ë…ì§€ìˆ˜ ìë™ ë¶„ì„ê¸°")

vocab_dict = load_vocab()
grade_ranges = load_grade_ranges()

input_method = st.radio("ì…ë ¥ ë°©ë²• ì„ íƒ", ("ë¬¸ì¥ ì…ë ¥", "ì´ë¯¸ì§€ ì—…ë¡œë“œ"))
trigger, text = False, ""

if input_method == "ë¬¸ì¥ ì…ë ¥":
    text = st.text_area("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”", key="manual")
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True
else:
    uploaded = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg", "heic"])
    if uploaded:
        image_bytes = uploaded.read()
        st.image(Image.open(uploaded), caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)
        ocr_result = call_vision_api(image_bytes).strip()
        st.session_state["ocr"] = ocr_result
    text = st.text_area("ğŸ“ OCR ê²°ê³¼ (ìˆ˜ì • ê°€ëŠ¥)", value=st.session_state.get("ocr", ""), key="ocr_text")
    if st.button("ğŸ” ë¶„ì„í•˜ê¸°"):
        trigger = True

if trigger or any(st.session_state.get(f"choice_{w}") for w in ambiguous_words):
    input_text = st.session_state.get("manual") if input_method == "ë¬¸ì¥ ì…ë ¥" else st.session_state.get("ocr_text")
    if input_text:
        user_choices = {}
        for word in ambiguous_words:
            if word in input_text:
                st.markdown(f"ğŸ” **â€˜{word}â€™ì˜ ì˜ë¯¸ë¥¼ ì„ íƒí•˜ì„¸ìš”:**")
                options = ambiguous_meanings[word]
                selected = st.radio(
                    f"{word} ì˜ë¯¸ ì„ íƒ", 
                    options=[("2", options["2"]), ("3", options["3"])],
                    format_func=lambda x: f"{x[0]}ë“±ê¸‰: {x[1]}", 
                    key=f"choice_{word}"
                )
                user_choices[word] = int(selected[0])

        score, level, used_words, total_count, total_words = calculate_onread_index(input_text, vocab_dict, grade_ranges, user_choices)

        if score == 0:
            st.warning(level)
        else:
            st.success(f"âœ… ì˜¨ë…ì§€ìˆ˜: {score}ì  ({level})")
            st.caption(f"ì´ ë‹¨ì–´ ìˆ˜: {total_words}, ì‚¬ê³ ë„êµ¬ì–´ ìˆ˜: {total_count}")
            if total_count < 3:
                st.info("ë¬¸ì¥ì´ ì§§ê±°ë‚˜ ì‚¬ê³ ë„êµ¬ì–´ê°€ ì ì–´ ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            if score > 500:
                st.info("ğŸ’¡ ê³ 3 ì´ìƒ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
            if used_words:
                st.markdown("### ì‚¬ìš©ëœ ì‚¬ê³ ë„êµ¬ì–´")
                for w, l in used_words:
                    st.markdown(f"- **{w}**: {l}ë“±ê¸‰")
    else:
        st.warning("â— ë¬¸ì¥ì„ ì…ë ¥í•œ ë’¤ 'ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
